# playwright_scraper.py
from playwright.sync_api import sync_playwright
import pandas as pd
import sqlite3
from datetime import datetime, timedelta
import time
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class PlaywrightStockScraper:
    def __init__(self):
        self.db_path = '/home/stock_prophet/data/stock_data.db'
        
    def scrape_yahoo_finance(self, ticker, days=90):
        """Yahoo Financeã‹ã‚‰æ ªä¾¡ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°"""
        logger.info(f"ğŸ­ Playwrightèµ·å‹•: {ticker}")
        
        with sync_playwright() as p:
            # Headless Chromiumèµ·å‹•
            browser = p.chromium.launch(
                headless=True,
                args=['--no-sandbox', '--disable-dev-shm-usage']
            )
            
            context = browser.new_context(
                user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            )
            
            page = context.new_page()
            
            try:
                # Yahoo Financeå±¥æ­´ãƒšãƒ¼ã‚¸
                url = f'https://finance.yahoo.com/quote/{ticker}/history'
                logger.info(f"ğŸ“Š ã‚¢ã‚¯ã‚»ã‚¹: {url}")
                
                page.goto(url, wait_until='networkidle', timeout=30000)
                
                # ãƒšãƒ¼ã‚¸ãŒå®Œå…¨ã«èª­ã¿è¾¼ã¾ã‚Œã‚‹ã¾ã§å¾…æ©Ÿ
                page.wait_for_selector('table', timeout=10000)
                
                # ãƒ†ãƒ¼ãƒ–ãƒ«ãƒ‡ãƒ¼ã‚¿å–å¾—
                rows = page.query_selector_all('table tbody tr')
                
                data = []
                for row in rows:
                    try:
                        cells = row.query_selector_all('td')
                        if len(cells) >= 7:
                            date_str = cells[0].inner_text()
                            open_price = cells[1].inner_text().replace(',', '')
                            high_price = cells[2].inner_text().replace(',', '')
                            low_price = cells[3].inner_text().replace(',', '')
                            close_price = cells[4].inner_text().replace(',', '')
                            adj_close = cells[5].inner_text().replace(',', '')
                            volume = cells[6].inner_text().replace(',', '')
                            
                            # ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
                            if open_price != '-' and close_price != '-':
                                data.append({
                                    'Date': date_str,
                                    'Open': float(open_price),
                                    'High': float(high_price),
                                    'Low': float(low_price),
                                    'Close': float(close_price),
                                    'Adj Close': float(adj_close),
                                    'Volume': int(volume) if volume != '-' else 0
                                })
                    except Exception as e:
                        logger.warning(f"è¡Œã®è§£æã‚¨ãƒ©ãƒ¼: {e}")
                        continue
                
                logger.info(f"âœ… {ticker}: {len(data)}ä»¶å–å¾—")
                
                # DataFrameã«å¤‰æ›
                df = pd.DataFrame(data)
                df['Date'] = pd.to_datetime(df['Date'])
                df = df.sort_values('Date')
                df.set_index('Date', inplace=True)
                
                return df
                
            except Exception as e:
                logger.error(f"âŒ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚¨ãƒ©ãƒ¼: {e}")
                return None
                
            finally:
                browser.close()
    
    def scrape_additional_info(self, ticker):
        """è¿½åŠ æƒ…å ±ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ï¼ˆãƒ‹ãƒ¥ãƒ¼ã‚¹ã€æŒ‡æ¨™ãªã©ï¼‰"""
        logger.info(f"ğŸ“° è¿½åŠ æƒ…å ±å–å¾—: {ticker}")
        
        with sync_playwright() as p:
            browser = p.chromium.launch(headless=True)
            page = browser.new_page()
            
            try:
                url = f'https://finance.yahoo.com/quote/{ticker}'
                page.goto(url, wait_until='networkidle')
                
                # ä¼æ¥­æƒ…å ±
                info = {}
                
                # æ™‚ä¾¡ç·é¡
                try:
                    market_cap = page.query_selector('[data-test="MARKET_CAP-value"]')
                    if market_cap:
                        info['market_cap'] = market_cap.inner_text()
                except:
                    pass
                
                # PER
                try:
                    pe_ratio = page.query_selector('[data-test="PE_RATIO-value"]')
                    if pe_ratio:
                        info['pe_ratio'] = pe_ratio.inner_text()
                except:
                    pass
                
                # æœ€æ–°ãƒ‹ãƒ¥ãƒ¼ã‚¹å–å¾—
                try:
                    news_items = page.query_selector_all('h3 a')
                    news = [item.inner_text() for item in news_items[:5]]
                    info['latest_news'] = news
                except:
                    pass
                
                logger.info(f"âœ… è¿½åŠ æƒ…å ±å–å¾—å®Œäº†")
                return info
                
            except Exception as e:
                logger.error(f"âŒ è¿½åŠ æƒ…å ±å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
                return {}
                
            finally:
                browser.close()
    
    def save_to_db(self, ticker, df):
        """SQLiteã«ä¿å­˜"""
        if df is not None and len(df) > 0:
            conn = sqlite3.connect(self.db_path)
            table_name = ticker.replace('.', '_').replace('-', '_')
            df.to_sql(table_name, conn, if_exists='replace')
            conn.close()
            logger.info(f"ğŸ’¾ DBä¿å­˜å®Œäº†: {table_name}")
    
    def run(self, tickers):
        """è¤‡æ•°éŠ˜æŸ„ã‚’é †æ¬¡ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°"""
        for ticker in tickers:
            try:
                # æ ªä¾¡ãƒ‡ãƒ¼ã‚¿
                df = self.scrape_yahoo_finance(ticker)
                if df is not None:
                    self.save_to_db(ticker, df)
                
                # è¿½åŠ æƒ…å ±
                info = self.scrape_additional_info(ticker)
                
                # ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–
                time.sleep(2)
                
            except Exception as e:
                logger.error(f"âŒ {ticker}å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
                continue

# å®Ÿè¡Œ
if __name__ == "__main__":
    scraper = PlaywrightStockScraper()
    
    tickers = [
        '7203.T',   # ãƒˆãƒ¨ã‚¿
        '6758.T',   # ã‚½ãƒ‹ãƒ¼
        '9984.T',   # ã‚½ãƒ•ãƒˆãƒãƒ³ã‚¯
        'AAPL',     # Apple
        'TSLA',     # Tesla
        'NVDA',     # NVIDIA
        'GOOGL',    # Google
        'MSFT',     # Microsoft
    ]
    
    scraper.run(tickers)
