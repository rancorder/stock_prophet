# playwright_scraper_optimized.py
from playwright.sync_api import sync_playwright
import pandas as pd
import sqlite3
from datetime import datetime
import logging
import gc  # ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class OptimizedStockScraper:
    def __init__(self):
        self.db_path = '/home/stock_prophet/data/stock_data.db'
        
    def scrape_with_single_browser(self, tickers):
        """1ã¤ã®ãƒ–ãƒ©ã‚¦ã‚¶ã§å…¨éŠ˜æŸ„ã‚’å‡¦ç†ï¼ˆãƒ¡ãƒ¢ãƒªç¯€ç´„ï¼‰"""
        logger.info("ğŸ­ Playwrightèµ·å‹•ï¼ˆæœ€é©åŒ–ãƒ¢ãƒ¼ãƒ‰ï¼‰")
        
        with sync_playwright() as p:
            # ãƒ–ãƒ©ã‚¦ã‚¶ã¯1å›ã ã‘èµ·å‹•
            browser = p.chromium.launch(
                headless=True,
                args=[
                    '--no-sandbox',
                    '--disable-dev-shm-usage',
                    '--disable-gpu',
                    '--disable-software-rasterizer',
                    '--disable-extensions',
                    # ãƒ¡ãƒ¢ãƒªç¯€ç´„è¨­å®š
                    '--single-process',
                    '--disable-background-networking',
                    '--disable-default-apps',
                    '--disable-sync',
                ]
            )
            
            context = browser.new_context(
                user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            )
            
            results = {}
            
            for ticker in tickers:
                try:
                    logger.info(f"ğŸ“Š å‡¦ç†ä¸­: {ticker}")
                    
                    # æ–°ã—ã„ãƒšãƒ¼ã‚¸ã‚’é–‹ã
                    page = context.new_page()
                    
                    # Yahoo Financeå±¥æ­´ãƒšãƒ¼ã‚¸
                    url = f'https://finance.yahoo.com/quote/{ticker}/history'
                    page.goto(url, wait_until='domcontentloaded', timeout=20000)
                    
                    # ãƒ†ãƒ¼ãƒ–ãƒ«å–å¾—ã‚’å¾…ã¤
                    try:
                        page.wait_for_selector('table tbody tr', timeout=10000)
                    except:
                        logger.warning(f"âš ï¸ {ticker}: ãƒ†ãƒ¼ãƒ–ãƒ«èª­ã¿è¾¼ã¿ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ")
                        page.close()
                        continue
                    
                    # ãƒ‡ãƒ¼ã‚¿æŠ½å‡º
                    rows = page.query_selector_all('table tbody tr')
                    
                    data = []
                    for row in rows[:90]:  # æœ€æ–°90æ—¥åˆ†ã®ã¿ï¼ˆãƒ¡ãƒ¢ãƒªç¯€ç´„ï¼‰
                        try:
                            cells = row.query_selector_all('td')
                            if len(cells) >= 7:
                                date_str = cells[0].inner_text()
                                
                                # "Dividend"è¡Œã¯ã‚¹ã‚­ãƒƒãƒ—
                                if 'Dividend' in date_str or 'Split' in date_str:
                                    continue
                                
                                open_price = cells[1].inner_text().replace(',', '')
                                high_price = cells[2].inner_text().replace(',', '')
                                low_price = cells[3].inner_text().replace(',', '')
                                close_price = cells[4].inner_text().replace(',', '')
                                adj_close = cells[5].inner_text().replace(',', '')
                                volume = cells[6].inner_text().replace(',', '')
                                
                                if open_price != '-' and close_price != '-':
                                    data.append({
                                        'Date': date_str,
                                        'Open': float(open_price),
                                        'High': float(high_price),
                                        'Low': float(low_price),
                                        'Close': float(close_price),
                                        'Adj Close': float(adj_close) if adj_close != '-' else float(close_price),
                                        'Volume': int(volume) if volume != '-' else 0
                                    })
                        except Exception as e:
                            continue
                    
                    if len(data) > 0:
                        df = pd.DataFrame(data)
                        df['Date'] = pd.to_datetime(df['Date'])
                        df = df.sort_values('Date')
                        df.set_index('Date', inplace=True)
                        
                        results[ticker] = df
                        logger.info(f"âœ… {ticker}: {len(df)}ä»¶å–å¾—")
                    else:
                        logger.warning(f"âš ï¸ {ticker}: ãƒ‡ãƒ¼ã‚¿ãªã—")
                    
                    # ãƒšãƒ¼ã‚¸ã‚’é–‰ã˜ã¦ãƒ¡ãƒ¢ãƒªè§£æ”¾
                    page.close()
                    
                    # çŸ­ã„å¾…æ©Ÿï¼ˆãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–ï¼‰
                    import time
                    time.sleep(1)
                    
                except Exception as e:
                    logger.error(f"âŒ {ticker}ã‚¨ãƒ©ãƒ¼: {e}")
                    continue
            
            browser.close()
            
            # æ˜ç¤ºçš„ã«ãƒ¡ãƒ¢ãƒªè§£æ”¾
            gc.collect()
            
            return results
    
    def save_all_to_db(self, results):
        """ä¸€æ‹¬ã§DBä¿å­˜"""
        conn = sqlite3.connect(self.db_path)
        
        for ticker, df in results.items():
            try:
                table_name = ticker.replace('.', '_').replace('-', '_')
                df.to_sql(table_name, conn, if_exists='replace')
                logger.info(f"ğŸ’¾ {ticker}ä¿å­˜å®Œäº†")
            except Exception as e:
                logger.error(f"âŒ {ticker}ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
        
        conn.close()
    
    def run(self, tickers):
        """å®Ÿè¡Œ"""
        results = self.scrape_with_single_browser(tickers)
        self.save_all_to_db(results)
        return results

# å®Ÿè¡Œ
if __name__ == "__main__":
    scraper = OptimizedStockScraper()
    
    # æ—¥æœ¬æ ª + ç±³å›½æ ª
    tickers = [
        # æ—¥æœ¬æ ª
        '7203.T',   # ãƒˆãƒ¨ã‚¿
        '6758.T',   # ã‚½ãƒ‹ãƒ¼
        '9984.T',   # ã‚½ãƒ•ãƒˆãƒãƒ³ã‚¯
        '6501.T',   # æ—¥ç«‹
        '8306.T',   # ä¸‰è±UFJ
        # ç±³å›½æ ª
        'AAPL',     # Apple
        'TSLA',     # Tesla
        'NVDA',     # NVIDIA
        'GOOGL',    # Google
        'MSFT',     # Microsoft
    ]
    
    logger.info("ğŸš€ æœ€é©åŒ–ç‰ˆã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼èµ·å‹•")
    results = scraper.run(tickers)
    logger.info(f"âœ… å®Œäº†: {len(results)}éŠ˜æŸ„")
