from playwright.sync_api import sync_playwright
import pandas as pd
import time
import logging
import sqlite3
import sys
sys.path.append('.')

logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.INFO)

class StockScraperFixed:
    def __init__(self):
        self.db_path = './data/stock_data.db'
    
    def scrape_single_stock(self, ticker):
        """1éŠ˜æŸ„ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ï¼ˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆå¯¾ç­–ç‰ˆï¼‰"""
        logger.info(f"ğŸ“Š å‡¦ç†é–‹å§‹: {ticker}")
        
        with sync_playwright() as p:
            browser = p.chromium.launch(
                headless=True,
                args=['--no-sandbox', '--disable-dev-shm-usage']
            )
            
            context = browser.new_context()
            page = context.new_page()
            
            try:
                url = f'https://finance.yahoo.com/quote/{ticker}/history'
                
                # ä¿®æ­£1: domcontentloadedã«å¤‰æ›´ï¼ˆé«˜é€ŸåŒ–ï¼‰
                # ä¿®æ­£2: ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ60ç§’ã«å»¶é•·
                page.goto(url, wait_until='domcontentloaded', timeout=60000)
                
                # ä¿®æ­£3: ãƒ†ãƒ¼ãƒ–ãƒ«å¾…æ©Ÿæ™‚é–“ã‚‚å»¶é•·
                page.wait_for_selector('table tbody tr', timeout=30000)
                
                rows = page.query_selector_all('table tbody tr')
                logger.info(f"  ãƒ†ãƒ¼ãƒ–ãƒ«è¡Œæ•°: {len(rows)}")
                
                data = []
                for row in rows[:90]:
                    try:
                        cells = row.query_selector_all('td')
                        if len(cells) >= 7:
                            date_str = cells[0].inner_text()
                            
                            if 'Dividend' in date_str or 'Split' in date_str:
                                continue
                            
                            open_p = cells[1].inner_text().replace(',', '')
                            high_p = cells[2].inner_text().replace(',', '')
                            low_p = cells[3].inner_text().replace(',', '')
                            close_p = cells[4].inner_text().replace(',', '')
                            volume = cells[6].inner_text().replace(',', '')
                            
                            if open_p != '-' and close_p != '-':
                                data.append({
                                    'Date': date_str,
                                    'Open': float(open_p),
                                    'High': float(high_p),
                                    'Low': float(low_p),
                                    'Close': float(close_p),
                                    'Volume': int(volume) if volume != '-' else 0
                                })
                    except Exception as e:
                        continue
                
                if len(data) > 0:
                    df = pd.DataFrame(data)
                    df['Date'] = pd.to_datetime(df['Date'])
                    df = df.sort_values('Date')
                    df.set_index('Date', inplace=True)
                    
                    logger.info(f"âœ… {ticker}: {len(df)}ä»¶å–å¾—")
                    return df
                else:
                    logger.warning(f"âš ï¸  {ticker}: ãƒ‡ãƒ¼ã‚¿ãªã—")
                    return None
                    
            except Exception as e:
                logger.error(f"âŒ {ticker}ã‚¨ãƒ©ãƒ¼: {str(e)[:100]}")
                return None
            finally:
                context.close()
                browser.close()
    
    def scrape_multiple(self, tickers):
        """è¤‡æ•°éŠ˜æŸ„ã‚’é †æ¬¡ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°"""
        results = {}
        
        for i, ticker in enumerate(tickers, 1):
            logger.info(f"\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
            logger.info(f"é€²æ—: {i}/{len(tickers)} - {ticker}")
            logger.info(f"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
            
            df = self.scrape_single_stock(ticker)
            
            if df is not None:
                results[ticker] = df
                self.save_to_db(ticker, df)
            else:
                logger.warning(f"âš ï¸  {ticker}ã®ãƒ‡ãƒ¼ã‚¿å–å¾—å¤±æ•—ï¼ˆã‚¹ã‚­ãƒƒãƒ—ï¼‰")
            
            # ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–ï¼ˆå¾…æ©Ÿæ™‚é–“ã‚’é•·ã‚ã«ï¼‰
            if i < len(tickers):
                logger.info(f"â³ 3ç§’å¾…æ©Ÿ...")
                time.sleep(3)
        
        return results
    
    def save_to_db(self, ticker, df):
        """SQLiteã«ä¿å­˜"""
        try:
            conn = sqlite3.connect(self.db_path)
            table_name = ticker.replace('.', '_').replace('-', '_')
            df.to_sql(table_name, conn, if_exists='replace')
            conn.close()
            logger.info(f"ğŸ’¾ DBä¿å­˜å®Œäº†: {table_name}")
        except Exception as e:
            logger.error(f"âŒ DBä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")

if __name__ == "__main__":
    from config.stock_config import get_all_tickers
    
    scraper = StockScraperFixed()
    tickers = get_all_tickers()
    
    logger.info(f"ğŸš€ æ”¹è‰¯ç‰ˆã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–‹å§‹")
    logger.info(f"ğŸ“Š å¯¾è±¡: {len(tickers)}éŠ˜æŸ„")
    logger.info(f"éŠ˜æŸ„: {', '.join(tickers)}")
    logger.info(f"")
    
    results = scraper.scrape_multiple(tickers)
    
    logger.info(f"\n" + "=" * 60)
    logger.info(f"âœ… ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Œäº†")
    logger.info(f"æˆåŠŸ: {len(results)}/{len(tickers)}éŠ˜æŸ„")
    logger.info(f"=" * 60)
    
    if len(results) > 0:
        logger.info(f"\nğŸ“Š å–å¾—æˆåŠŸéŠ˜æŸ„:")
        for ticker in results.keys():
            logger.info(f"  âœ… {ticker}")
