"""
æ ªä¾¡ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ï¼ˆplaywrightä½¿ç”¨ï¼‰
Yahoo Financeã‹ã‚‰æ ªä¾¡ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
"""
from playwright.sync_api import sync_playwright
import pandas as pd
import time
import logging
import sqlite3
from typing import Optional, Dict

logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.INFO)

class StockScraperFixed:
    """æ ªä¾¡ãƒ‡ãƒ¼ã‚¿ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼"""
    
    def __init__(self):
        """åˆæœŸåŒ–"""
        self.db_path = './data/stock_data.db'
    
    def scrape_single_stock(
        self,
        ticker: str,
        days: int = 90
    ) -> Optional[pd.DataFrame]:
        """
        å˜ä¸€éŠ˜æŸ„ã®æ ªä¾¡ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°
        
        Args:
            ticker: ãƒ†ã‚£ãƒƒã‚«ãƒ¼ã‚·ãƒ³ãƒœãƒ«ï¼ˆä¾‹: 'AAPL', '7203.T'ï¼‰
            days: å–å¾—æ—¥æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 90æ—¥ï¼‰
        
        Returns:
            æ ªä¾¡ãƒ‡ãƒ¼ã‚¿ã®DataFrameã€å¤±æ•—æ™‚ã¯None
            columns: ['Date', 'Open', 'High', 'Low', 'Close', 'Volume']
        
        Examples:
            >>> scraper = StockScraperFixed()
            >>> df = scraper.scrape_single_stock('AAPL')
            >>> print(df.head())
        """
        logger.info(f"ğŸ“Š å‡¦ç†é–‹å§‹: {ticker}")
        
        with sync_playwright() as p:
            browser = p.chromium.launch(
                headless=True,
                args=[
                    '--no-sandbox',
                    '--disable-dev-shm-usage',
                    '--disable-gpu',
                ]
            )
            
            page = browser.new_page()
            
            try:
                url = f'https://finance.yahoo.com/quote/{ticker}/history'
                page.goto(url, wait_until='domcontentloaded', timeout=60000)
                page.wait_for_selector('table tbody tr', timeout=20000)
                
                rows = page.query_selector_all('table tbody tr')
                
                data = []
                for row in rows[:days]:
                    try:
                        cells = row.query_selector_all('td')
                        if len(cells) >= 7:
                            date_str = cells[0].inner_text()
                            
                            if 'Dividend' in date_str or 'Split' in date_str:
                                continue
                            
                            open_p = cells[1].inner_text().replace(',', '')
                            high_p = cells[2].inner_text().replace(',', '')
                            low_p = cells[3].inner_text().replace(',', '')
                            close_p = cells[4].inner_text().replace(',', '')
                            volume = cells[6].inner_text().replace(',', '')
                            
                            if open_p != '-' and close_p != '-':
                                data.append({
                                    'Date': date_str,
                                    'Open': float(open_p),
                                    'High': float(high_p),
                                    'Low': float(low_p),
                                    'Close': float(close_p),
                                    'Volume': int(volume) if volume != '-' else 0
                                })
                    except:
                        continue
                
                if len(data) > 0:
                    df = pd.DataFrame(data)
                    df['Date'] = pd.to_datetime(df['Date'])
                    df = df.sort_values('Date')
                    df.set_index('Date', inplace=True)
                    
                    logger.info(f"âœ… {ticker}: {len(df)}ä»¶å–å¾—")
                    return df
                else:
                    logger.warning(f"âš ï¸  {ticker}: ãƒ‡ãƒ¼ã‚¿ãªã—")
                    return None
                    
            except Exception as e:
                logger.error(f"âŒ {ticker}ã‚¨ãƒ©ãƒ¼: {e}")
                return None
            finally:
                browser.close()
    
    def scrape_multiple(
        self,
        tickers: list[str]
    ) -> Dict[str, pd.DataFrame]:
        """
        è¤‡æ•°éŠ˜æŸ„ã‚’é †æ¬¡ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°
        
        Args:
            tickers: ãƒ†ã‚£ãƒƒã‚«ãƒ¼ã‚·ãƒ³ãƒœãƒ«ã®ãƒªã‚¹ãƒˆ
        
        Returns:
            {ticker: DataFrame} ã®è¾æ›¸
        """
        results = {}
        
        for i, ticker in enumerate(tickers, 1):
            logger.info(f"\né€²æ—: {i}/{len(tickers)}")
            
            df = self.scrape_single_stock(ticker)
            
            if df is not None:
                results[ticker] = df
                self.save_to_db(ticker, df)
            
            if i < len(tickers):
                time.sleep(3)
        
        return results
    
    def save_to_db(
        self,
        ticker: str,
        df: pd.DataFrame
    ) -> None:
        """
        SQLiteã«ãƒ‡ãƒ¼ã‚¿ä¿å­˜
        
        Args:
            ticker: ãƒ†ã‚£ãƒƒã‚«ãƒ¼ã‚·ãƒ³ãƒœãƒ«
            df: æ ªä¾¡ãƒ‡ãƒ¼ã‚¿ã®DataFrame
        """
        try:
            conn = sqlite3.connect(self.db_path)
            table_name = ticker.replace('.', '_').replace('-', '_')
            df.to_sql(table_name, conn, if_exists='replace')
            conn.close()
            logger.info(f"ğŸ’¾ DBä¿å­˜å®Œäº†: {table_name}")
        except Exception as e:
            logger.error(f"âŒ DBä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")

if __name__ == "__main__":
    import sys
    sys.path.append('.')
    from config.stock_config import get_all_tickers
    
    scraper = StockScraperFixed()
    tickers = get_all_tickers()
    
    logger.info(f"ğŸš€ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–‹å§‹: {len(tickers)}éŠ˜æŸ„")
    results = scraper.scrape_multiple(tickers)
    logger.info(f"âœ… å®Œäº†: {len(results)}/{len(tickers)}éŠ˜æŸ„")
